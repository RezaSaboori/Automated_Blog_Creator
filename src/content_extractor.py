# -*- coding: utf-8 -*-
"""content_extractor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cIJgnYQbEd5bkTBWh3K0XXZBrGDMuuuP
"""

# Importing Required Libraries
import feedparser
import requests
from pymongo import MongoClient
from datetime import datetime
from bs4 import BeautifulSoup
#from readability import Document
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import random
import time
from .proxy_checker import create_session, generate_user_agents

# Function to Extract Main Content from a Web Page
def extract_main_content(url, proxy):
    session = create_session()
    while True:
        try:
            proxy = proxy
            headers = {'User-Agent': generate_user_agents()}
            proxies = {'http': proxy, 'https': proxy} if proxy else {}
            response = session.get(url, headers=headers, proxies=proxies, timeout=10)
            if response.status_code == 200:
                doc = Document(response.text)
                soup = BeautifulSoup(doc.summary(), 'html.parser')
                main_content = soup.get_text(separator='\n', strip=True)
                return main_content
            else:
                print(f'Request failed with status code: {response.status_code}, for URL: {url}')
                if proxy is None:
                    return None
        except StopIteration:
            try:
                headers = {'User-Agent': generate_user_agents()}
                response = session.get(url, headers=headers,  timeout=10)
                if response.status_code == 200:
                    doc = Document(response.text)
                    soup = BeautifulSoup(doc.summary(), 'html.parser')
                    main_content = soup.get_text(separator='\n', strip=True)
                    return main_content
                else:
                    print(f'Request failed with status code: {response.status_code}, for URL: {url}')
                    return None
            except requests.RequestException as e:
                print(f'Request exception: {e}, for URL: {url}')
                return None
        except requests.RequestException as e:
            print(f'Request exception: {e}, for URL: {url}')
            return None

#Choose a valid proxy for each RSS feed
def choose_proxy(url, proxy_generator):
    session = create_session()
    while True:
        try:
            proxy = next(proxy_generator)
            proxies = {'http': proxy, 'https': proxy} if proxy else {}
            headers = {'User-Agent': generate_user_agents()}
            print(f"Fetching {url} using proxy: {proxy}")
            response = session.get(url, headers=headers, proxies=proxies, timeout=3)
            if response.status_code == 200:
                print(f"Using proxy: {proxy} for feed: {url}")
                return proxy
            else:
                print(f'Request failed with status code: {response.status_code}, for feed: {url}')
                if proxy is None:
                    return None
        except StopIteration:
            print('No more proxies available. Trying without proxy...')
            try:
                headers = {'User-Agent': generate_user_agents()}
                response = session.get(url, headers=headers,  timeout=3)
                if response.status_code == 200:
                    return proxy
                else:
                    print(f'Request failed with status code: {response.status_code}, for feed: {url}')
                    return None
            except requests.RequestException as e:
                print(f'Request exception: {e}, for feddL: {url}')
                return None
        except requests.exceptions.ProxyError:
            print(f'Proxy error with proxy: {proxy}. Trying next proxy...')
        except requests.RequestException as e:
            print(f'Request exception: {e}, for feed: {url}')
            return None

# Function to Parse RSS Feed and Store Articles in MongoDB
def parse_rss_feed(url, proxy):
    feed = feedparser.parse(url)
    total_urls = len(feed.entries)
    success_count = 0
    for entry in feed.entries:
        title = entry.title
        link = entry.link
        main_content = extract_main_content(link, proxy)
        if main_content:
            success_count += 1
            article_doc = {
                "title": title,
                "link": link,
                "content": main_content
            }
            articles_collection.insert_one(article_doc)
        else:
            print(f"Skipping feed entry: {title}")
    print(f"Successfully fetched {success_count} out of {total_urls} URLs for feed: {url}")
